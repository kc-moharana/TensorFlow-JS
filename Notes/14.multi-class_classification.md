# Multi-class classification

- we may have n-classes
- so we have n-outputs
  + each output willbe between 0-1.
  + one-hot encoding
  + sum of all the output will be 1.
  + highest class is the winner.
  + winning is a likelihood. so represented as %


## one hot encoding
- since we have n-outputs we have to select one node as a single output
- in one hot encoding one one node is "hot"
  + other node as "cold"
  + example:

No. of classes(n) | class-1 |class-2  |class-3  |class-4  |class-5  |
------------------|---|---|---|---|---|
3  |1,0,0|0,1,0|0,0,1|NA|NA|
4  |1,0,0,0|0,1,0,0|0,0,1,0|0,0,0,1|NA|
5  |1,0,0,0,0|0,1,0,0,0|0,0,1,0,0|0,0,0,1,0|0,0,0,0,1|

- in our house class problem, we will use house size + price to predict number of bedrooms (1/2/3+) in that house (on a percentage basis).
  + we have 3 class problem or n=3.
  + one hot encoding: (1,0,0), (0,1,0), (0,0,1)

```javascript
function getClassIndex(className) {
  if (className === 1 || className === "1") {
    return 0; // 1 bedroom
  }
  else if (className === 2 || className === "2") {
    return 1; // 2 bedrooms
  }
  else {
    return 2; // 3+ bedrooms
  }
}

function getClassName(classIndex) {
  if (classIndex === 2) {
    return "3+";
  }
  else {
    return classIndex + 1;
  }
}
```

- `tf.oneHot(indices, depth)` to create one-hot encodings using tensorflowJS.
  + Creates a one-hot `tf.Tensor` for indices of class=depth

```javascript
// creating labels tensors

const features_values = points.map(p=>[p.x, p.y]);
const features_tensor = tf.tensor2d(features_values);

const label_values = points.map(p=>getClassIndex(p.class));
const labels_tensor = tf.oneHot(tf.tensor1d( label_values, 'int32'), 3 );
// const labels_tensor = tf.tidy(()=>{tf.oneHot(tf.tensor1d( label_values, 'int32'), 3 ) });


```


## Multi-class model

### `softmax` activation function
- other two layers are using sigmoid activation function which outputs values between 0-1
- so the final output from 3 output nodes will have a sum of 0-3
  + one of them will have higher value (winner): 0.3, 0.4, 0.3
- softmax will make this output to 0-1 which is desirable for our output


### `categorical Cross entropy` loss function
- it penalizes the confident but incorrect outputs 


```javascript
function createModel () {
    model = tf.sequential();

    model.add(tf.layers.dense({
      units: 10,
      useBias: true,
      activation: 'sigmoid',
      inputDim: 2,
    }));
    model.add(tf.layers.dense({
      units: 10,
      useBias: true,
      activation: 'sigmoid',
    }));
    model.add(tf.layers.dense({
      units: 3,                   // output of 3 nodes
      useBias: true,
      activation: 'softmax',    // softmax: sum
    }));

    const optimizer = tf.train.adam();
    model.compile({
      loss: 'categoricalCrossentropy',
      optimizer,
    });

    return model;
  }
```
